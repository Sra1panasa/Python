{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled28.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM01JAjAWEd8PZdRnHkz5O3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sra1panasa/Python/blob/main/pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pyspark:\n",
        "\n",
        "* PySpark is a tool created by Apache Spark Community for using Python with Spark. It allows working with RDD (Resilient Distributed Dataset) in Python. \n",
        "* It also offers PySpark Shell to link Python APIs with Spark core to initiate Spark Context. Spark is the name engine to realize cluster computing, while PySpark is Python’s library to use Spark.\n",
        "\n",
        "* ***In PySpark, SparkContext is available as sc by default, so creating a new SparkContext will throw an error.***"
      ],
      "metadata": {
        "id": "24BMnvBT-1Vc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Parameters***\n",
        "\n",
        "* SparkContext has some parameters that are listed below:\n",
        "\n",
        "* **Master**: The URL of the cluster SparkContext connects to\n",
        "* **AppName**: The name of your job\n",
        "* SparkHome: A Spark installation directory\n",
        "* PyFiles: The .zip or .py files send to the cluster and then added to PYTHONPATH\n",
        "* Environment: Worker node environment variables\n",
        "* BatchSize: The number of Python objects represented. However, to disable  batching, set the value to 1; to automatically choose the batch size based on the object size, set it to 0; and to use an unlimited batch size, set it to −1\n",
        "* Serializer: This parameter tells about an RDD serializer\n",
        "* Conf: An object of L{SparkConf} to set all Spark properties\n",
        "* profiler_cls: A class of custom profilers used to do profiling; however, \n",
        " * pyspark.profiler.BasicProfiler is the default one"
      ],
      "metadata": {
        "id": "f1m5caOxT2UU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Data scientist main’s job is to analyze and build predictive models. In short, a data scientist needs to know how to query data using SQL, produce a statistical report and make use of machine learning to produce predictions.\n",
        "\n",
        "* Data scientist spends a significant amount of their time on cleaning, transforming and analyzing the data. Once the dataset or data workflow is ready, the data scientist uses various techniques to discover insights and hidden patterns. The data manipulation should be robust and the same easy to use. Spark is the right tool thanks to its speed and rich APIs."
      ],
      "metadata": {
        "id": "-ct62rLPAod_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ***SparkContext is the internal engine that allows the connections with the clusters. If you want to run an operation, you need a SparkContext.***"
      ],
      "metadata": {
        "id": "QhnMfz9iBX_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "sc =SparkContext()"
      ],
      "metadata": {
        "id": "P2YaU29rDl5-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Now that the SparkContext is ready, you can create a collection of data called RDD, Resilient Distributed Dataset. Computation in an RDD is automatically parallelized across the cluster.**"
      ],
      "metadata": {
        "id": "_2MpjL15DVCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nums= sc.parallelize([1,2,3,4])\n",
        "nums.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2CeIT2CDxiR",
        "outputId": "30653b72-4b3e-43ab-8abd-c5e05c346494"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#You can apply a transformation to the data with a lambda function. \n",
        "#In the PySpark example below, you return the square of nums. It is a map transformation\n",
        "\n",
        "squared = nums.map(lambda x: x*x).collect()\n",
        "for num in squared:\n",
        "    print('%i ' % (num))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiJctgrnEKJC",
        "outputId": "a21d035d-d243-43f1-c35f-957294c15f85"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 \n",
            "4 \n",
            "9 \n",
            "16 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.guru99.com/pyspark-tutorial.html\n",
        "\n",
        "* here you can find muliple operations related to spark dataframe..\n",
        "\n"
      ],
      "metadata": {
        "id": "YGGVfHkHEAiQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark SQL Tutorial\n",
        "\n",
        "https://sparkbyexamples.com/pyspark-tutorial/\n",
        "\n",
        "* ***PySpark SQL is one of the most used PySpark modules which is used for processing structured columnar data format. Once you have a DataFrame created, you can interact with the data by using SQL syntax.***\n",
        "\n",
        "* In other words, Spark SQL brings native RAW SQL queries on Spark meaning you can run traditional ANSI SQL’s on Spark Dataframe, in the later section of this PySpark SQL tutorial, you will learn in detail using SQL select, where, group by, join, union e.t.c\n",
        "\n",
        "* In order to use SQL, first, create a temporary table on DataFrame using createOrReplaceTempView() function. Once created, this table can be accessed throughout the SparkSession using sql() and it will be dropped along with your SparkContext termination.\n",
        "\n",
        "* Use sql() method of the SparkSession object to run the query and this method returns a new DataFrame."
      ],
      "metadata": {
        "id": "EibOe9AlFwPn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ***Dataframe  operations examples***\n",
        "* Different ways to Create DataFrame in PySpark\n",
        "* PySpark – Ways to Rename column on DataFrame\n",
        "* PySpark withColumn() usage with Examples\n",
        "* PySpark – How to Filter data from DataFrame\n",
        "* PySpark orderBy() and sort() explained\n",
        "* PySpark explode array and map columns to rows\n",
        "* PySpark – explode nested array into rows\n",
        "* PySpark Read CSV file into DataFrame\n",
        "* PySpark Groupby Explained with Examples\n",
        "* PySpark Aggregate Functions with Examples\n",
        "* PySpark Joins Explained with Examples"
      ],
      "metadata": {
        "id": "NEkKzEFJGKNO"
      }
    }
  ]
}